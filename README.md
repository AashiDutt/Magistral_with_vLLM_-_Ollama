# Magistral_with_vLLM_&_Ollama

This repo contains two notebooks - Magistral_vLLM and Magistral_Ollama

Magistral_vLLM - Uses vLLM that runs on Runpod.io on a A100 GPU

Magistral_Ollama - Runs locally within a single RTX 4090 or a 32GB RAM MacBook once quantized.
